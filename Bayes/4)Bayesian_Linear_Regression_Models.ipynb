{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pymc3/tuning/starting.py:61: UserWarning: find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.\n",
      "  warnings.warn('find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.')\n"
     ]
    }
   ],
   "source": [
    "##https://www.quantstart.com/articles/Bayesian-Linear-Regression-Models-with-PyMC3/\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set(style=\"darkgrid\", palette=\"muted\")\n",
    "\n",
    "\n",
    "def simulate_linear_data(N, beta_0, beta_1, eps_sigma_sq):\n",
    "    \"\"\"\n",
    "    Simulate a random dataset using a noisy\n",
    "    linear process.\n",
    "\n",
    "    N: Number of data points to simulate\n",
    "    beta_0: Intercept\n",
    "    beta_1: Slope of univariate predictor, X\n",
    "    \"\"\"\n",
    "    # Create a pandas DataFrame with column 'x' containing\n",
    "    # N uniformly sampled values between 0.0 and 1.0\n",
    "    df = pd.DataFrame(\n",
    "        {\"x\": \n",
    "            map(lambda x: float(x)/100.0,\n",
    "                np.random.RandomState(42).choice(\n",
    "                101, N, replace=False)\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Use a linear model (y ~ beta_0 + beta_1*x + epsilon) to \n",
    "    # generate a column 'y' of responses based on 'x'\n",
    "    eps_mean = 0.0\n",
    "    df[\"y\"] = beta_0 + beta_1*df[\"x\"] + np.random.RandomState(42).normal(\n",
    "        eps_mean, eps_sigma_sq, N\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def glm_mcmc_inference(df, iterations=5000):\n",
    "    \"\"\"\n",
    "    Calculates the Markov Chain Monte Carlo trace of\n",
    "    a Generalised Linear Model Bayesian linear regression \n",
    "    model on supplied data.\n",
    "\n",
    "    df: DataFrame containing the data\n",
    "    iterations: Number of iterations to carry out MCMC for\n",
    "    \"\"\"\n",
    "    # Use PyMC3 to construct a model context\n",
    "    basic_model = pm.Model()\n",
    "    with basic_model:\n",
    "        # Create the glm using the Patsy model syntax\n",
    "        # We use a Normal distribution for the likelihood\n",
    "        pm.glm.GLM.from_formula(\"y ~ x\", df, family=pm.glm.families.Normal())\n",
    "\n",
    "        # Use Maximum A Posteriori (MAP) optimisation as initial value for MCMC\n",
    "        start = pm.find_MAP()\n",
    "\n",
    "        # Use the No-U-Turn Sampler\n",
    "        step = pm.NUTS()\n",
    "\n",
    "        # Calculate the trace\n",
    "        trace = pm.sample(\n",
    "            iterations, step, start, \n",
    "            random_seed=42, progressbar=True\n",
    "        )\n",
    "\n",
    "    return trace\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # These are our \"true\" parameters\n",
    "    beta_0 = 1.0  # Intercept\n",
    "    beta_1 = 2.0  # Slope\n",
    "\n",
    "    # Simulate 100 data points, with a variance of 0.5\n",
    "    N = 100\n",
    "    eps_sigma_sq = 0.5\n",
    "\n",
    "    # Simulate the \"linear\" data using the above parameters\n",
    "    df = simulate_linear_data(N, beta_0, beta_1, eps_sigma_sq)\n",
    "\n",
    "    # Plot the data, and a frequentist linear regression fit\n",
    "    # using the seaborn package\n",
    "    sns.lmplot(x=\"x\", y=\"y\", data=df, size=10)\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    \n",
    "    trace = glm_mcmc_inference(df, iterations=5000)\n",
    "    pm.traceplot(trace[500:])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot a sample of posterior regression lines\n",
    "    sns.lmplot(x=\"x\", y=\"y\", data=df, height=10, fit_reg=False)\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    plt.ylim(0.0, 4.0)\n",
    "    pm.plot_posterior_predictive_glm(trace, samples=100)\n",
    "    x = np.linspace(0, 1, N)\n",
    "    y = beta_0 + beta_1*x\n",
    "    plt.plot(x, y, label=\"True Regression Line\", lw=3., c=\"green\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
